groups:
  - name: common
    rules:

      - alert: VolumeFreeDiskSpace
        expr: round(100 - ((node_filesystem_avail_bytes{device=~".*DO_Volume_pvc.*"} * 100) / node_filesystem_size_bytes{device=~".*DO_Volume_pvc.*"})) > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Volume almost full on {{ $labels.instance }}"
          description: "Node {{ $labels.instance }} attached volume {{ $labels.device }} almost full ({{ $value }}%)"

      - alert: NodeFreeDiskSpace
        expr: round(100 - ((node_filesystem_avail_bytes{device=~"/dev/vda.+", mountpoint="/"} * 100) / node_filesystem_size_bytes{device=~"/dev/vda.+", mountpoint="/"})) > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.instance }} disk almost full"
          description: "Disk {{ $labels.device }} is almost full ({{ $value }}%)"

      - alert: ApiPodDown
        expr: sum(kube_pod_status_ready{namespace="cash-track", condition="true", pod=~"api.*"}) < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: API down
          description: API service does not have enough ready pods to serve traffic for more than 5 minutes

      - alert: WebsitePodDown
        expr: sum(kube_pod_status_ready{namespace="cash-track", condition="true", pod=~"website.*"}) < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Website down
          description: Website service does not have enough ready pods to serve traffic for more than 5 minutes

      - alert: FrontendPodDown
        expr: sum(kube_pod_status_ready{namespace="cash-track", condition="true", pod=~"frontend.*"}) < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Frontend down
          description: Frontend service does not have enough ready pods to serve traffic for more than 5 minutes

      - alert: DatabasePodDown
        expr: sum(kube_pod_status_ready{namespace="cash-track", condition="true", pod="mysql-0"}) < 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Database down
          description: Database stateful set pod is not in ready state for more than 5 minutes

      - alert: DatabaseBackupPodDown
        expr: sum(kube_pod_status_ready{namespace="cash-track", condition="true", pod=~"mysql-backup-.*"}) < 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Database down
          description: Database stateful set pod is not in ready state for more than 5 minutes

      - alert: PodScheduleFailed
        expr: sum(kube_pod_status_phase{namespace=~"telegram-bots|monitoring|ingress-nginx|cash-track", phase="Pending"}) > 1
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.pod }} schedule failed"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} cannot be scheduled to any node for last 30 minutes"

      - alert: ContainerTooManyRestarts
        expr: sum by (pod, container) (changes(kube_pod_container_status_restarts_total{namespace=~"telegram-bots|monitoring|ingress-nginx|cash-track"}[5m])) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.container }} too frequently restarting"
          description: "Container {{ $labels.container }} of a pod {{ $labels.pod }} restarted more than 5 times by last 5 minutes"

      - alert: NodeHighCPURequest
        expr: (sum by (node) (kube_pod_container_resource_requests{resource="cpu"}) / sum by (node) (kube_node_status_allocatable{resource="cpu"}) * 100) > 98
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.node }} high CPU request"
          description: "Node {{ $labels.node }} is under CPU request more than 95% of allocatable resources"

      - alert: NodeHighMemoryRequest
        expr: (sum by (node) (kube_pod_container_resource_requests{resource="memory"}) / sum by (node) (kube_node_status_allocatable{resource="memory"}) * 100) > 98
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.node }} high memory request"
          description: "Node {{ $labels.node }} is under memory request more than 95% of allocatable resources"

      - alert: HTTPServiceHighServerErrorRate
        expr: round((sum by (service, method, path, status) (rate(nginx_ingress_controller_requests{service=~"api|website|frontend|crashers-bot",status=~"5.."}[2m])) / sum by (service, method, path, status) (rate(nginx_ingress_controller_requests{service=~"api|website|frontend|crashers-bot"}[2m]))) * 100) > 20
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "HTTP service {{ $labels.service }} has reached {{ $value }}% of 5xx responses rate"
          description: "Service {{ $labels.service }} responding to {{ $labels.method }} {{ $labels.path }} with HTTP status code {{ $labels.status }} for more than {{ $value }}% requests for last 5 minutes"

      - alert: HTTPServiceHighClientErrorRate
        expr: round((sum by (service, method, path, status) (rate(nginx_ingress_controller_requests{service=~"api|website|frontend|crashers-bot",status=~"4..",status!="404"}[2m])) / sum by (service, method, path, status) (rate(nginx_ingress_controller_requests{service=~"api|website|frontend|crashers-bot"}[2m]))) * 100) > 80
        for: 5m
        labels:
          severity: info
        annotations:
          summary: "HTTP service {{ $labels.service }} has reached {{ $value }}% of 4xx responses rate"
          description: "Service {{ $labels.service }} responding to {{ $labels.method }} {{ $labels.path }} with HTTP status code {{ $labels.status }} for more than {{ $value }}% requests for last 5 minutes"

      - alert: HTTPServiceHighResponseLatency
        expr: histogram_quantile(0.90, sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{service=~"api|website|frontend|crashers-bot"}[2m])) by (service, le, method, path)) > 1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "HTTP service {{ $labels.service }} response time is high"
          description: "Service {{ $labels.service }} responds to {{ $labels.method }} {{ $labels.path }} with high latency for more than {{ $value }} seconds for last 2 minutes"

  - "name": "tempo_rules"
    "rules":
      - "expr": "histogram_quantile(0.99, sum(rate(tempo_request_duration_seconds_bucket[1m])) by (le, cluster, namespace, job, route))"
        "record": "cluster_namespace_job_route:tempo_request_duration_seconds:99quantile"
      - "expr": "histogram_quantile(0.50, sum(rate(tempo_request_duration_seconds_bucket[1m])) by (le, cluster, namespace, job, route))"
        "record": "cluster_namespace_job_route:tempo_request_duration_seconds:50quantile"
      - "expr": "sum(rate(tempo_request_duration_seconds_sum[1m])) by (cluster, namespace, job, route) / sum(rate(tempo_request_duration_seconds_count[1m])) by (cluster, namespace, job, route)"
        "record": "cluster_namespace_job_route:tempo_request_duration_seconds:avg"
      - "expr": "sum(rate(tempo_request_duration_seconds_bucket[1m])) by (le, cluster, namespace, job, route)"
        "record": "cluster_namespace_job_route:tempo_request_duration_seconds_bucket:sum_rate"
      - "expr": "sum(rate(tempo_request_duration_seconds_sum[1m])) by (cluster, namespace, job, route)"
        "record": "cluster_namespace_job_route:tempo_request_duration_seconds_sum:sum_rate"
      - "expr": "sum(rate(tempo_request_duration_seconds_count[1m])) by (cluster, namespace, job, route)"
        "record": "cluster_namespace_job_route:tempo_request_duration_seconds_count:sum_rate"

  - "name": "tempo_alerts"
    "rules":
      - "alert": "TempoCompactorUnhealthy"
        "annotations":
          "message": "There are {{ printf \"%f\" $value }} unhealthy compactor(s)."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoCompactorUnhealthy"
        "expr": |
          max by (cluster, namespace) (tempo_ring_members{state="Unhealthy", name="compactor", namespace=~".*"}) > 0
        "for": "15m"
        "labels":
          "severity": "critical"
      - "alert": "TempoDistributorUnhealthy"
        "annotations":
          "message": "There are {{ printf \"%f\" $value }} unhealthy distributor(s)."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoDistributorUnhealthy"
        "expr": |
          max by (cluster, namespace) (tempo_ring_members{state="Unhealthy", name="distributor", namespace=~".*"}) > 0
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "TempoIngesterUnhealthy"
        "annotations":
          "message": "There are {{ printf \"%f\" $value }} unhealthy ingester(s)."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoIngesterUnhealthy"
        "expr": |
          max by (cluster, namespace) (tempo_ring_members{state="Unhealthy", name="ingester", namespace=~".*"}) > 0
        "for": "15m"
        "labels":
          "severity": "critical"
      - "alert": "TempoMetricsGeneratorUnhealthy"
        "annotations":
          "message": "There are {{ printf \"%f\" $value }} unhealthy metric-generator(s)."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoMetricsGeneratorUnhealthy"
        "expr": |
          max by (cluster, namespace) (tempo_ring_members{state="Unhealthy", name="metrics-generator", namespace=~".*"}) > 0
        "for": "15m"
        "labels":
          "severity": "critical"
      - "alert": "TempoCompactionsFailing"
        "annotations":
          "message": "Greater than 2 compactions have failed in the past hour."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoCompactionsFailing"
        "expr": |
          sum by (cluster, namespace) (increase(tempodb_compaction_errors_total{}[1h])) > 2 and
          sum by (cluster, namespace) (increase(tempodb_compaction_errors_total{}[5m])) > 0
        "for": "1h"
        "labels":
          "severity": "critical"
      - "alert": "TempoIngesterFlushesUnhealthy"
        "annotations":
          "message": "Greater than 2 flush retries have occurred in the past hour."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoIngesterFlushesFailing"
        "expr": |
          sum by (cluster, namespace) (increase(tempo_ingester_failed_flushes_total{}[1h])) > 2 and
          sum by (cluster, namespace) (increase(tempo_ingester_failed_flushes_total{}[5m])) > 0
        "for": "5m"
        "labels":
          "severity": "warning"
      - "alert": "TempoIngesterFlushesFailing"
        "annotations":
          "message": "Greater than 2 flush retries have failed in the past hour."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoIngesterFlushesFailing"
        "expr": |
          sum by (cluster, namespace) (increase(tempo_ingester_flush_failed_retries_total{}[1h])) > 2 and
          sum by (cluster, namespace) (increase(tempo_ingester_flush_failed_retries_total{}[5m])) > 0
        "for": "5m"
        "labels":
          "severity": "critical"
      - "alert": "TempoPollsFailing"
        "annotations":
          "message": "Greater than 2 polls have failed in the past hour."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoPollsFailing"
        "expr": |
          sum by (cluster, namespace) (increase(tempodb_blocklist_poll_errors_total{}[1h])) > 2 and
          sum by (cluster, namespace) (increase(tempodb_blocklist_poll_errors_total{}[5m])) > 0
        "labels":
          "severity": "critical"
      - "alert": "TempoTenantIndexFailures"
        "annotations":
          "message": "Greater than 2 tenant index failures in the past hour."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoTenantIndexFailures"
        "expr": |
          sum by (cluster, namespace) (increase(tempodb_blocklist_tenant_index_errors_total{}[1h])) > 2 and
          sum by (cluster, namespace) (increase(tempodb_blocklist_tenant_index_errors_total{}[5m])) > 0
        "labels":
          "severity": "critical"
      - "alert": "TempoNoTenantIndexBuilders"
        "annotations":
          "message": "No tenant index builders for tenant {{ $labels.tenant }}. Tenant index will quickly become stale."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoNoTenantIndexBuilders"
        "expr": |
          sum by (cluster, namespace, tenant) (tempodb_blocklist_tenant_index_builder{}) == 0 and
          max by (cluster, namespace) (tempodb_blocklist_length{}) > 0
        "for": "5m"
        "labels":
          "severity": "critical"
      - "alert": "TempoTenantIndexTooOld"
        "annotations":
          "message": "Tenant index age is 600 seconds old for tenant {{ $labels.tenant }}."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoTenantIndexTooOld"
        "expr": |
          max by (cluster, namespace, tenant) (tempodb_blocklist_tenant_index_age_seconds{}) > 600
        "for": "5m"
        "labels":
          "severity": "critical"
      - "alert": "TempoBlockListRisingQuickly"
        "annotations":
          "message": "Tempo block list length is up 40 percent over the last 7 days.  Consider scaling compactors."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoBlockListRisingQuickly"
        "expr": |
          avg(tempodb_blocklist_length{namespace=~".*", container="compactor"}) / avg(tempodb_blocklist_length{namespace=~".*", container="compactor"} offset 7d) by (cluster, namespace) > 1.4
        "for": "15m"
        "labels":
          "severity": "critical"
      - "alert": "TempoBadOverrides"
        "annotations":
          "message": "{{ $labels.job }} failed to reload overrides."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoBadOverrides"
        "expr": |
          sum(tempo_runtime_config_last_reload_successful{namespace=~".*"} == 0) by (cluster, namespace, job)
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "TempoUserConfigurableOverridesReloadFailing"
        "annotations":
          "message": "Greater than 5 user-configurable overides reloads failed in the past hour."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoTenantIndexFailures"
        "expr": |
          sum by (cluster, namespace) (increase(tempo_overrides_user_configurable_overrides_reload_failed_total{}[1h])) > 5 and
          sum by (cluster, namespace) (increase(tempo_overrides_user_configurable_overrides_reload_failed_total{}[5m])) > 0
        "labels":
          "severity": "critical"
      - "alert": "TempoProvisioningTooManyWrites"
        "annotations":
          "message": "Ingesters in {{ $labels.cluster }}/{{ $labels.namespace }} are receiving more data/second than desired, add more ingesters."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoProvisioningTooManyWrites"
        "expr": |
          avg by (cluster, namespace) (rate(tempo_ingester_bytes_received_total{job=~".+/ingester"}[5m])) / 1024 / 1024 > 30
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "TempoCompactorsTooManyOutstandingBlocks"
        "annotations":
          "message": "There are too many outstanding compaction blocks in {{ $labels.cluster }}/{{ $labels.namespace }} for tenant {{ $labels.tenant }}, increase compactor's CPU or add more compactors."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoCompactorsTooManyOutstandingBlocks"
        "expr": |
          sum by (cluster, namespace, tenant) (tempodb_compaction_outstanding_blocks{container="compactor", namespace=~".*"}) / ignoring(tenant) group_left count(tempo_build_info{container="compactor", namespace=~".*"}) by (cluster, namespace) > 100
        "for": "6h"
        "labels":
          "severity": "warning"
      - "alert": "TempoCompactorsTooManyOutstandingBlocks"
        "annotations":
          "message": "There are too many outstanding compaction blocks in {{ $labels.cluster }}/{{ $labels.namespace }} for tenant {{ $labels.tenant }}, increase compactor's CPU or add more compactors."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoCompactorsTooManyOutstandingBlocks"
        "expr": |
          sum by (cluster, namespace, tenant) (tempodb_compaction_outstanding_blocks{container="compactor", namespace=~".*"}) / ignoring(tenant) group_left count(tempo_build_info{container="compactor", namespace=~".*"}) by (cluster, namespace) > 250
        "for": "24h"
        "labels":
          "severity": "critical"
      - "alert": "TempoIngesterReplayErrors"
        "annotations":
          "message": "Tempo ingester has encountered errors while replaying a block on startup in {{ $labels.cluster }}/{{ $labels.namespace }} for tenant {{ $labels.tenant }}"
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoIngesterReplayErrors"
        "expr": |
          sum by (cluster, namespace, tenant) (increase(tempo_ingester_replay_errors_total{namespace=~".*"}[5m])) > 0
        "for": "5m"
        "labels":
          "severity": "critical"
      - "alert": "TempoPartitionLagWarning"
        "annotations":
          "message": "Tempo partition {{ $labels.partition }} in consumer group {{ $labels.group }} is lagging by more than 300 seconds in {{ $labels.cluster }}/{{ $labels.namespace }}."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoPartitionLag"
        "expr": |
          max by (cluster, namespace, group, partition) (tempo_ingest_group_partition_lag_seconds{namespace=~".*", group=~"metrics-generator|block-builder"}) > 300
        "for": "5m"
        "labels":
          "severity": "warning"
      - "alert": "TempoPartitionLagCritical"
        "annotations":
          "message": "Tempo partition {{ $labels.partition }} in consumer group {{ $labels.group }} is lagging by more than 900 seconds in {{ $labels.cluster }}/{{ $labels.namespace }}."
          "runbook_url": "https://github.com/grafana/tempo/tree/main/operations/tempo-mixin/runbook.md#TempoPartitionLag"
        "expr": |
          max by (cluster, namespace, group, partition) (tempo_ingest_group_partition_lag_seconds{namespace=~".*", group=~"metrics-generator|block-builder"}) > 900
        "for": "5m"
        "labels":
          "severity": "critical"
